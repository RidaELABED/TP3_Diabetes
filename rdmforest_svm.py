# -*- coding: utf-8 -*-
"""rdmForest_SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XH8rYhXYin876vwJiPnG8O6ILqkXPhIJ
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import csv
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import BaggingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import AdaBoostClassifier

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/My Drive/DL"

data =  pd.read_csv('diabetes.csv', sep=",")

data.head()

data_save = data.copy()

x = data.drop(['Outcome'],axis = 1)
x.head()

y = data['Outcome']
y.head()

sns.set()

sns.pairplot(data, hue="Outcome", height=5);

xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.9, random_state = 0)

clf = tree.DecisionTreeClassifier()
clf.fit(xtrain, ytrain)
Z = clf.predict(xtest)
print(clf.score(xtest, ytest))

accu_tree = []

for _ in range(100):
    clf = tree.DecisionTreeClassifier()
    clf.fit(xtrain, ytrain)
    Z = clf.predict(xtest)
    accu_tree.append(clf.score(xtest, ytest))

variance_tree = np.var(accu_tree)
print(variance_tree)

clf = BaggingClassifier(tree.DecisionTreeClassifier(), 
                            max_samples=0.5, max_features=0.5, 
                            n_estimators=200)
clf.fit(xtrain, ytrain)
Z = clf.predict(xtest)
print(clf.score(xtest, ytest))

accu_bagging = []

for _ in range(100):
    clf = BaggingClassifier(tree.DecisionTreeClassifier(), 
                            max_samples=0.5, max_features=0.5, 
                            n_estimators=200)
    clf.fit(xtrain, ytrain)
    Z = clf.predict(xtest)
    accu_bagging.append(clf.score(xtest, ytest))

variance_bagging = np.var(accu_bagging)
print(variance_bagging)

accu_bagging_i = []
x_ax = []

for i in range(10, 500, 5):
    clf = BaggingClassifier(tree.DecisionTreeClassifier(), 
                            max_samples=0.5, max_features=0.5, 
                            n_estimators=i)
    clf.fit(xtrain, ytrain)
    Z = clf.predict(xtest)
    accu_bagging_i.append(clf.score(xtest, ytest))
    x_ax.append(i)

plt.plot(x_ax, accu_bagging_i)
plt.title('Accuracy VS N_estimator')
plt.xlabel('n_estimator')
plt.ylabel('accuracy')

clf = BaggingClassifier(tree.DecisionTreeClassifier(), 
                            max_samples=0.5, max_features=0.5, 
                            n_estimators=10)
clf.fit(xtrain, ytrain)

parameters = {"max_samples": [5, 10, 20, 50, 100, 150, 200, 250, 300, 400, 500],
              "max_features": [5, 10, 20, 50, 100, 150, 200, 250, 300, 400, 500]}
GsCV = GridSearchCV(BaggingClassifier(tree.DecisionTreeClassifier(), n_estimators=200),
                                      parameters,
                                      scoring='neg_mean_squared_error',
                                      cv=5)

GsCV.fit(xtrain, ytrain)

print(GsCV.best_params_)

clf = BaggingClassifier(tree.DecisionTreeClassifier(), 
                            max_samples=20, max_features=5, 
                            n_estimators=150)
clf.fit(xtrain, ytrain)
Z = clf.predict(xtest)
print(clf.score(xtest, ytest))

clf = RandomForestClassifier(n_estimators=200)
clf.fit(xtrain, ytrain)

ypred = clf.predict(xtest)
accuracy = clf.score(xtest, ytest)
print(accuracy)

clf = RandomForestClassifier(n_estimators=200)
clf.fit(xtrain, ytrain)
Z = clf.predict(xtest)
print(clf.score(xtest, ytest))

accu_rdmForest = []

for _ in range(100):
    clf = RandomForestClassifier(n_estimators=200)
    clf.fit(xtrain, ytrain)
    Z = clf.predict(xtest)
    accu_rdmForest.append(clf.score(xtest, ytest))

variance_rdmForest = np.var(accu_rdmForest)
print(variance_rdmForest)

accu_rdmForest_i = []
x_ax = []

for i in range(10, 500, 5):
    clf = RandomForestClassifier(n_estimators=i)
    clf.fit(xtrain, ytrain)
    Z = clf.predict(xtest)
    accu_rdmForest_i.append(clf.score(xtest, ytest))
    x_ax.append(i)

plt.plot(x_ax, accu_rdmForest_i)
plt.title('Accuracy VS N_estimator')
plt.xlabel('n_estimator')
plt.ylabel('accuracy')

clf = ExtraTreesClassifier(n_estimators=200)
clf.fit(xtrain, ytrain)
accuracy = clf.score(xtest, ytest)
print(accuracy)

accu_xtraForest = []

for _ in range(100):
    clf = ExtraTreesClassifier(n_estimators=200)
    clf.fit(xtrain, ytrain)
    Z = clf.predict(xtest)
    accu_xtraForest.append(clf.score(xtest, ytest))

variance_xtraForest = np.var(accu_xtraForest)
print(variance_xtraForest)

accu_xtraForest_i = []
x_ax = []

for i in range(10, 500, 5):
    clf = ExtraTreesClassifier(n_estimators=i)
    clf.fit(xtrain, ytrain)
    Z = clf.predict(xtest)
    accu_xtraForest_i.append(clf.score(xtest, ytest))
    x_ax.append(i)

plt.plot(x_ax, accu_xtraForest_i)
plt.title('Accuracy VS N_estimator')
plt.xlabel('n_estimator')
plt.ylabel('accuracy')

clf = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=5),
                         n_estimators=200, learning_rate=2)
clf.fit(xtrain, ytrain)
accuracy = clf.score(xtest, ytest)
print(accuracy)

parameters = {"learning_rate" : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}

GsCV = GridSearchCV(AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=5), n_estimators=200),
                    parameters,
                    scoring='neg_mean_squared_error',
                    cv=5)

GsCV.fit(xtrain, ytrain)

print(GsCV.best_params_)

from sklearn import svm
from sklearn.preprocessing import StandardScaler

X = data.values[:, 2:4]

Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, test_size = 0.9, random_state = 0)

C = 1.0 # paramètre de régularisation
lin_svc = svm.LinearSVC(C=C, loss="hinge", random_state=0)
lin_svc.fit(Xtrain, Ytrain)

score = lin_svc.score(Xtest, Ytest)

print(f"score : {score}")

# Créer la surface de décision discretisée
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
# Pour afficher la surface de décision on va discrétiser l'espace avec un pas h
h = max((x_max - x_min) / 100, (y_max - y_min) / 100)
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
# Surface de décision
Z = lin_svc.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)
# Afficher aussi les points d'apprentissage
plt.scatter(Xtrain[:, 0], Xtrain[:, 1], label="train", edgecolors='k', c=Ytrain, cmap=plt.cm.coolwarm)
plt.scatter(Xtest[:, 0], Xtest[:, 1], label="test", marker='*', c=Ytest, cmap=plt.cm.coolwarm)
plt.title("LinearSVC")

for i in range(8):
    for j in range(i+1, 8):
        X = data.values[:, [i, j]]

        scaler = StandardScaler()
        scaler.fit(X, y)
        X = scaler.transform(X)

        Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, test_size = 0.9, random_state = 0)

        C = 1.0
        lin_svc = svm.LinearSVC(C=C, loss="hinge", random_state=0)
        lin_svc.fit(Xtrain, Ytrain)

        score = lin_svc.score(Xtest, Ytest)

        # Créer la surface de décision discretisée
        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        # Pour afficher la surface de décision on va discrétiser l'espace avec un pas h
        h = max((x_max - x_min) / 100, (y_max - y_min) / 100)
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        # Surface de décision
        Z = lin_svc.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        plt.figure()
        plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)
        # Afficher aussi les points d'apprentissage
        plt.scatter(Xtrain[:, 0], Xtrain[:, 1], label="train", edgecolors='k', c=Ytrain, cmap=plt.cm.coolwarm)
        plt.scatter(Xtest[:, 0], Xtest[:, 1], label="test", marker='*', c=Ytest, cmap=plt.cm.coolwarm)
        plt.xlabel(data.columns[i])
        plt.ylabel(data.columns[j])
        plt.title(f'Paire : {data.columns[i]} et {data.columns[j]}\n Score : {score}')

C=10

lin_svc = svm.LinearSVC(C=C, loss="squared_hinge").fit(xtrain, ytrain)
score = lin_svc.score(xtest, ytest)

print(f"score : {score}")

from sklearn import svm
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.pipeline import Pipeline


X = data.values[:, [1, 5]]

xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.5, random_state=0)

print(X)

C = 1.0 # paramètre de régularisation
lin_svc = Pipeline([('scaler',StandardScaler()),
                    ('linear_svc', svm.LinearSVC(C=1, loss="hinge"))])
lin_svc.fit(xtrain, ytrain)

score = lin_svc.score(xtest, ytest)
print(score)

sc = []

for C in range(1, 100):
  lin_svc = Pipeline([('scaler',StandardScaler()),
                    ('linear_svc', svm.LinearSVC(C=C, loss="hinge"))])
  lin_svc.fit(xtrain, ytrain)

  score = lin_svc.score(xtest, ytest)
  sc.append(score)

plt.plot(sc)
plt.title('Score en fontion du coefficient de régularisation')
plt.xlabel('C')
plt.ylabel('score')

lin_svc = Pipeline([('scaler',StandardScaler()),
                    ('linear_svc', svm.LinearSVC(C=20, loss="hinge"))])
lin_svc.fit(xtrain, ytrain)

score = lin_svc.score(xtest, ytest)
print(score)

for C in range(1, 1000, 100):
  lin_svc =  Pipeline([('scaler',StandardScaler()),
                    ('linear_svc', svm.LinearSVC(C=C, loss="hinge"))])
  lin_svc.fit(xtrain, ytrain)

  score = lin_svc.score(xtest, ytest)

  # Créer la surface de décision discretisée
  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
  # Pour afficher la surface de décision on va discrétiser l'espace avec un pas h
  h = max((x_max - x_min) / 100, (y_max - y_min) / 100)
  xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
  # Surface de décision
  Z = lin_svc.predict(np.c_[xx.ravel(), yy.ravel()])
  Z = Z.reshape(xx.shape)
  plt.figure()
  plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)
  # Afficher aussi les points d'apprentissage
  plt.scatter(xtrain[:, 0], xtrain[:, 1], label="train", edgecolors='k', c=ytrain, cmap=plt.cm.coolwarm)
  plt.scatter(xtest[:, 0], xtest[:, 1], label="test", marker='*', c=ytest, cmap=plt.cm.coolwarm)
  plt.title(f'Paire : Glucose et BMI\n C = {C}')

x = data.drop(['Outcome'],axis = 1)
xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.5, random_state=0)

tuned_parameters = {'C': [1,2,3,4,5,6,7,8,9]}

clf = GridSearchCV(svm.SVC(), tuned_parameters, cv=5)
# exécution de grid search
clf.fit(xtrain, ytrain)

print(clf.best_params_)

svm_clf = Pipeline([('scaler',StandardScaler()),
                    ('linear_svc', svm.LinearSVC(C=3, loss='hinge'))])
svm_clf.fit(xtrain, ytrain)
svm_clf.score(xtest, ytest)

